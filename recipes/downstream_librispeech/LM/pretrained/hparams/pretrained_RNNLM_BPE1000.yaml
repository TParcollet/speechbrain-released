# ############################################################################
# Model: LM for LibriSpeech
# Tokens: Pre-trained LibriSpeech tokens
# losses: NLL
# Training: Timers and Such
# Authors:  Loren Lugosch 2020
# ############################################################################

save_folder: model_checkpoints
lm_ckpt_file: /gpfswork/rech/kxg/uyk21ll/datasets/librispeech/speechbrain_lm/rnnlm_bpe1000/model.ckpt
tokenizer_file: /gpfswork/rech/kxg/uyk21ll/datasets/librispeech/speechbrain_lm/rnnlm_bpe1000/1000_unigram.model
device: 'cuda:0'

# Model params
emb_size: 128
num_asr_tokens: 1000

model: !new:speechbrain.lobes.models.RNNLM.RNNLM
    output_neurons: !ref <num_asr_tokens>
    embedding_dim: !ref <emb_size>
    activation: !name:torch.nn.LeakyReLU
    dropout: 0.0
    rnn_layers: 2
    rnn_neurons: 2048
    dnn_blocks: 1
    dnn_neurons: 512
    return_hidden: True  # For inference

tokenizer: !new:recipes.LibriSpeech.Tokenizer.pretrained.pretrained.tokenizer
    tokenizer_file: !ref <tokenizer_file>
    save_folder: !ref <save_folder>
